# ğŸ“˜ Generalized Pell Equation Solutions Analysis

A comprehensive exploration of integer solutions to the generalized Pell equation:

<p align="center"><code><strong>xÂ² âˆ’ DyÂ² = N</strong></code></p>

for nonâ€‘square integers `D` and arbitrary integers `N`.

# Statistical Analysis of the Generalized Pell's Equation

### Abstract
This report presents a comprehensive framework and Python implementation for the rigorous statistical analysis of datasets containing integer solutions to the generalized Pell's equation, $x^2 - Dy^2 = N$. Moving beyond deterministic, single-equation solutions, we treat the problem as a statistical ensemble of equations, characterized by the parameters $(D, N)$. We introduce methodologies for isolating fundamental solutions, analyzing their distributional properties, and building predictive models for their magnitude and for the number of solution classes. The accompanying Python code provides a complete, extensible toolkit for researchers exploring the statistical landscape of Diophantine equations.

---

### Section 1: Foundational Framework: From Deterministic Solutions to a Statistical Ensemble

This section establishes the conceptual bridge between the deterministic, algebraic nature of Pell's equation and the probabilistic lens of statistics. It argues that while any single solution is calculable, the collective behavior of solutions across many equations can reveal deep structural patterns.

#### 1.1 The Statistical Paradox of Deterministic Data

At first glance, the application of statistical methods to the solutions of Pell-type equations presents a conceptual paradox. The solutions $(x, y)$ to any given equation $x^2 - Dy^2 = N$ are not random; they are fully determined by the integer parameters $D$ and $N$ and can be generated by deterministic algorithms, such as the PQa algorithm or the Lagrange-Matthews-Mollin (LMM) algorithm. There is no stochastic process, no measurement error, and no sampling from a larger population in the traditional sense.

However, the value of a statistical approach emerges when one shifts perspective. Instead of viewing each solution as an isolated, deterministic outcome, we can consider the entire dataset of solutions, generated from a range of $D$ and $N$ values, as a **statistical ensemble**. This reframing is motivated by a well-documented phenomenon in number theory: the apparent unpredictability of solution characteristics. For the simple Pell equation $x^2 - Dy^2 = 1$, the magnitude of the smallest positive solution can vary wildly for adjacent values of $D$, with no simple, discernible pattern. This "lack of an easy relationship" between the inputs $(D, N)$ and the outputs (the size of the solutions) is precisely the domain where statistical modeling excels.

Our analysis, therefore, is not concerned with predicting the outcome of a single roll of a die. Rather, it is analogous to the methods of statistical mechanics. In a volume of gas, the trajectory of any individual molecule is, in principle, governed by deterministic laws of motion. Yet, predicting these individual paths is intractable and unenlightening. Instead, statistical mechanics describes the collective, emergent properties of the systemâ€”such as temperature and pressureâ€”which are distributions of molecular energies and momenta. Similarly, we will not attempt to predict a single solution with probabilistic uncertainty. Instead, we will model the distribution of solution characteristics across the ensemble of equations. We seek to answer questions such as: *What is the typical magnitude of a fundamental solution? Does this magnitude follow a recognizable probability distribution? Can we build a model that explains a significant portion of the variance in solution size based on the properties of $D$ and $N$?* By treating the collection of equations as an ensemble, we can uncover deep structural patterns and statistical regularities that are invisible when studying equations one at a time.

#### 1.2 The Algebraic Structure of the Solution Space

A naive statistical analysis of a raw dataset of solutions to $x^2 - Dy^2 = N$ would be fundamentally misleading. The solutions are not independent and identically distributed (IID) random variables, a common assumption in many statistical techniques. Instead, they possess a rich algebraic structure that must be respected. The set of all solutions for a given $(D, N)$ pair is partitioned into a finite number of **equivalence classes**.

Two solutions, $(x_1, y_1)$ and $(x_2, y_2)$, are defined as equivalent if one can be generated from the other through multiplication by a solution to the corresponding simple Pell equation, $t^2 - Du^2 = 1$. Specifically, if $(t, u)$ is any integer solution to $t^2 - Du^2 = 1$, and $(r, s)$ is a solution to $x^2 - Dy^2 = N$, then $(rt + suD, ru + st)$ is also a solution to $x^2 - Dy^2 = N$. This relationship can be expressed more elegantly in the quadratic field $Q(\sqrt{D})$: if $\alpha = r + s\sqrt{D}$ and $\epsilon = t + u\sqrt{D}$ are elements whose norms correspond to solutions, then their product, $\alpha\epsilon$, also corresponds to a solution.

All solutions within a single equivalence class can be generated from one representative member of that class. A particularly important representative is the **fundamental solution**. For a given class, the fundamental solution is typically defined as the solution $(x, y)$ with the smallest non-negative value of $y$ (with a tie-breaking rule for $x \ge 0$). Let $(r, s)$ be the fundamental solution of a class and let $(t, u)$ be the minimal positive solution (also called the fundamental solution) to $t^2 - Du^2 = 1$. Then all other solutions $(x_n, y_n)$ in that class can be generated by the formula:

$$x_n + y_n\sqrt{D} = (r + s\sqrt{D})(t + u\sqrt{D})^n$$

for all integers $n$.

This structure has profound implications for statistical analysis. Within a class, the solutions are perfectly correlated and their magnitudes grow exponentially with the index $n$. A raw dataset containing multiple solutions from the same class would be heavily skewed by the large, structurally redundant solutions generated for high values of $n$. Any statistical measure, such as the mean or variance of $x$, would be dominated by these few enormous values and would be meaningless.

Therefore, the primary unit of our statistical analysis cannot be the individual solution $(x, y)$. It must be the **fundamental solution** of each equivalence class. By first processing the raw data to identify the unique set of fundamental solutions, we create a new dataset where each data point represents an entire family of solutions. This set of fundamental solutions is the proper foundation for rigorous statistical inquiry, as it captures the essential, independent information about the solution space for each equation.

#### 1.3 The Logarithmic Transformation: Linearizing Exponential Growth

The solutions to Pell's equation can be extraordinarily large. The minimal solution to $x^2 - 991y^2 = 1$, for instance, has an $x$ value with 30 digits. Such a vast range of magnitudes poses a severe challenge for most statistical and visualization techniques. The distribution of raw solution sizes is pathologically skewed, with most values clustered near zero and a few astronomical outliers.

The algebraic structure once again provides the key to managing this scale. The generative formula for solutions within a class is multiplicative. In statistics and data analysis, the standard tool for handling multiplicative processes and exponential growth is the **logarithmic transformation**. Applying the natural logarithm to the generation formula linearizes the relationship:

$$\log(x_n + y_n\sqrt{D}) = \log(r + s\sqrt{D}) + n \cdot \log(t + u\sqrt{D})$$

For large solutions, $x$ and $y$ are related by the asymptotes of the hyperbola $x^2 - Dy^2 = N$, meaning $x \approx y\sqrt{D}$. Therefore, $x + y\sqrt{D} \approx 2x$. This leads to the approximation:

$$\log(x_n) \approx \frac{1}{2}\log(r + s\sqrt{D}) + \frac{n}{2}\log(t + u\sqrt{D})$$

This reveals that the logarithms of consecutive solutions within a class are approximately equally spaced. This transformation achieves two critical goals. First, it tames the extreme skew of the data, compressing the vast range of magnitudes into a more manageable, human-comprehensible scale. Second, and more fundamentally, it converts the underlying multiplicative structure into an additive one, which is the natural domain for linear models and many standard statistical distributions (like the Normal distribution). Throughout this report, we will primarily analyze the logarithms of the fundamental solutions, $\log(x_{\text{fund}})$ and $\log(y_{\text{fund}})$, as this approach is not merely a numerical convenience but a reflection of the deep algebraic nature of the problem.

## ğŸ“‚ Repository Contents

- **Dataset**: Monte Carloâ€“generated CSV of `(D, N, x, y)` with `num_classes` (number of equivalence classes).
- **Notebooks**: Google Colab notebooks for data cleaning, visualization, statistical analysis, outlier detection, clustering, and ML modeling.
- **Results**: Key insights and rich figures that capture the behavior of solutions and highlight prediction challenges.

---

## ğŸ“‘ Dataset Overview

- **Trials**: 1000 randomly generated `(D, N)` pairs.
- **Columns**:
  - `D`: Positive non-square integer (2â€“99)
  - `N`: Non-zero integer (âˆ’100 to +100)
  - `num_classes`: Number of equivalence classes of integer solutions for the pair `(D, N)`
  - `x`, `y`: One representative solution per class (extended using the fundamental unit)

- **Zero-solution cases**: 824 entries have no solution (`num_classes = 0`, `x = y = 0`).

---

## ğŸ” Exploratory Data Analysis

### 1. Distributions

- `D` and `N` values are nearly uniform across their ranges.
- `num_classes` distribution is heavily skewed toward 2 â€” most solvable cases yield exactly two equivalence classes.
- The values of `logâ‚â‚€|x|` and `logâ‚â‚€|y|` exhibit a long right tail:
  - Most solutions are modest.
  - A few extend to hundreds of digits.

### 2. Correlations

- `log|x|` and `log|y|`: near-perfect correlation.
- Moderate correlation between `D` and the size of minimal solutions.
- Weak or negligible linear correlation of `(D, N)` with `num_classes`.

### 3. Scatter & Heatmaps

- Scatterplot of `(D, N)` colored by `num_classes` shows no clear clustering.
- Heatmap of pairwise feature correlations reveals strong grouping only among solution magnitude features.

---

## ğŸ“Š Visualizations

> _All figures are saved in the `/figures/` folder or embedded in the Colab notebooks._

- Histograms of `D`, `N`, and `num_classes`
- Density plots of `logâ‚â‚€|x|` and `logâ‚â‚€|y|`
- Scatterplots of `(D, N)` â†’ `num_classes`
- Correlation heatmaps
- Boxplots illustrating solution size spread

---

## âš ï¸ Outliers & Anomalies

- **Rare high class counts** (5 or 6 classes) are found in cases like:
  - `(5, -76)`, `(53, -52)`, `(58, 63)`, `(61, 52)`, `(85, -76)`
- **Extreme solution sizes**:
  - Example: `(D, N) = (61, 52)` produces minimal `(x, y)` with ~200 digits
- These are not data errors, but true mathematical outliers.

---

## ğŸ§® Regression & Clustering Models

- **Regression on `(D, N)` â†’ `num_classes`**:
  - Linear: RÂ² â‰ˆ 0.01
  - Quadratic: RÂ² â‰ˆ 0.07

- **Clustering (K-means / DBSCAN)** on:
  - `(D, N)` â€” separates trivial (no-solution) from solvable, but not by class count.
  - `(log|x|, log|y|)` â€” more meaningful clustering by solution size.

---

## ğŸ¤– Predictive Modeling

- **Goal**: Predict `num_classes` using `(D, N, x, y)`-related features.
- **Features used**: `D`, `N`, `logâ‚â‚€|x|`, `logâ‚â‚€|y|`
- **Models & Results**:
  - Decision Tree: ~99.7% accuracy
  - Random Forest: ~99.9% accuracy
- **Note**: These models memorize known `(D, N)` labels. Generalization to unseen inputs remains extremely difficult due to number-theoretic complexity.

---

## ğŸ’¡ Key Insights

- **Minimal solution sizes are unpredictable** â€” adjacent `D` values may have drastically different-sized solutions.
- **Most solvable `(D, N)` yield exactly 2 classes**, confirming theoretical predictions.
- **`D` and `N` alone are weak predictors** of class count or solution size.
- **Machine learning models can overfit** without truly capturing mathematical structure.

---

## ğŸ“– References

1. Robertson, J.â€¯P. (2004). *Solving the Generalized Pell Equation*.
2. Matthews, K.â€¯R. (2000). *Diophantine Equations and the LMM Algorithm*.
3. Niven, Zuckerman & Montgomery (1991). *An Introduction to the Theory of Numbers*.
4. Mollin, R.â€¯E. (1998). *Fundamental Number Theory with Applications*.

---

## ğŸš€ Getting Started

1. Clone this repository:
   ```bash
   git clone https://github.com/your-username/pell-equation-analysis.git
   cd pell-equation-analysis
